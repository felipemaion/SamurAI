import os
import re

import logging
from datetime import datetime
from typing import List

from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class Document:
    id: str
    filename: str
    content: str
    chunks: List[str]
    metadata: dict


def normalize_whitespace(text: str) -> str:
    """
    Normaliza espa√ßos em branco em um texto:
    - Remove espa√ßos duplicados
    - Remove tabs e newlines extras
    """
    # troca tabs e quebras por espa√ßo
    text = re.sub(r"[\t\n\r]+", " ", text)
    # remove m√∫ltiplos espa√ßos
    text = re.sub(r" {2,}", " ", text)
    # remove espa√ßos no in√≠cio/fim
    return text.strip()


class DocumentProcessor:
    """Classe para processar documentos PDF e gerar chunks por senten√ßa com spaCy"""

    def __init__(self, nlp, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

        # Carrega modelo spaCy para portugu√™s
        try:
            self.nlp = nlp
            logger.info("‚úÖ spaCy carregado com modelo pt_core_news_sm")
        except OSError:
            logger.error(
                "‚ö†Ô∏è Modelo spaCy pt_core_news_sm n√£o encontrado. "
                "Execute: python -m spacy download pt_core_news_sm"
            )
            raise

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extrai texto de um arquivo PDF"""
        import pypdf

        try:
            with open(pdf_path, "rb") as file:
                reader = pypdf.PdfReader(file)
                text = ""
                for page in reader.pages:
                    text += page.extract_text() + "\n"
                return text
        except Exception as e:
            logger.error(f"Erro ao extrair texto do PDF {pdf_path}: {e}")
            return ""

    def chunk_text(self, text: str) -> List[str]:
        """
        Divide o texto em chunks de no m√°ximo `chunk_size` palavras,
        respeitando senten√ßas com spaCy.
        """
        doc = self.nlp(text)
        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]

        chunks = []
        current_chunk = []
        current_length = 0

        for sent in sentences:
            sent_len = len(sent.split())

            # Se a pr√≥xima senten√ßa cabe no chunk atual
            if current_length + sent_len <= self.chunk_size:
                current_chunk.append(sent)
                current_length += sent_len
            else:
                # Salva chunk atual
                chunks.append(normalize_whitespace(" ".join(current_chunk)))
                # Inicia pr√≥ximo chunk com overlap opcional
                if self.chunk_overlap > 0 and len(current_chunk) > 0:
                    overlap_words = []
                    # Pega √∫ltimas palavras do chunk para overlap
                    for s in reversed(current_chunk):
                        overlap_words = s.split() + overlap_words
                        if len(overlap_words) >= self.chunk_overlap:
                            break
                    current_chunk = [" ".join(overlap_words)]
                    current_length = len(overlap_words)
                else:
                    current_chunk = []
                    current_length = 0
                # adiciona a senten√ßa que n√£o coube
                current_chunk.append(sent)
                current_length += sent_len

        # adiciona o √∫ltimo chunk
        if current_chunk:
            chunks.append(normalize_whitespace(" ".join(current_chunk)))

        logger.info(f"üìÑ Texto dividido em {len(chunks)} chunks")
        return chunks

    def process_document(self, pdf_path: str) -> Document:
        """Processa um documento PDF completo"""
        filename = os.path.basename(pdf_path)
        content = self.extract_text_from_pdf(pdf_path)
        if not content:
            logger.warning(f"Nenhum conte√∫do extra√≠do de {filename}")
            return None

        # Normaliza conte√∫do
        content = normalize_whitespace(content)

        # Inclui t√≠tulo + categoria
        categoria = os.path.basename(os.path.dirname(pdf_path))
        content_with_title = f"T√≠tulo do documento: {filename.replace('.pdf', '')}, categorias:{categoria}\n\n{content}"

        # Normaliza novamente (caso concatenar t√≠tulo tenha adicionado espa√ßos extras)
        content_with_title = normalize_whitespace(content_with_title)

        chunks = self.chunk_text(content_with_title)

        document = Document(
            id=filename,
            filename=filename,
            content=content_with_title,
            chunks=chunks,
            metadata={
                "path": pdf_path,
                "processed_at": datetime.now().isoformat(),
                "num_chunks": len(chunks),
                "categoria": categoria,
            },
        )

        return document
